{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the LightRL Documentation","text":"<p>Welcome to the official documentation for LightRL, a lightweight and efficient Reinforcement Learning library aimed at providing simple yet powerful tools for developing and experimenting with RL algorithms.</p>"},{"location":"#overview","title":"Overview","text":"<p>LightRL is designed to be user-friendly and flexible, providing researchers and developers with the essential tools to build and test their RL models. Whether you're a beginner or an experienced practitioner, LightRL aims to smooth the learning curve and increase productivity.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Modular Design: Easily plug and play with different algorithms and environments.</li> <li>Scalable &amp; Efficient: Optimized for performance, making it suitable for both research and production environments.</li> <li>Easy Integration: Works seamlessly with popular machine learning frameworks and libraries.</li> <li>Comprehensive Examples: Step-by-step guides and example scripts to help you get started quickly.</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>To start using LightRL, check out the following resources:</p> <ul> <li>Installation Guide: Instructions on how to install LightRL and its dependencies.</li> <li>Quick Start: A brief introduction to using LightRL for your first project.</li> <li>API Documentation: Detailed information on the API and available functions.</li> <li>Examples: In-depth tutorials and examples showcasing the features and capabilities of LightRL.</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions! Whether it\u2019s reporting a bug, suggesting new features, or contributing code, we appreciate your support. Please check out our Contributing Guidelines for more information.</p>"},{"location":"#license","title":"License","text":"<p>LightRL is licensed under the MIT License, ensuring that it remains free and open-source.</p>"},{"location":"#contact","title":"Contact","text":"<p>For questions, suggestions, or feedback, you can reach out to us at email@example.com or open an issue on our GitHub repository.</p> <p>Thank you for choosing LightRL! We hope it helps you in your journey through reinforcement learning.</p>"},{"location":"api/","title":"Reference","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>lightrl/bandits.py</code> <pre><code>class Bandit(ABC):\n    def __init__(self, arms: List[Any]) -&gt; None:\n        \"\"\"\n        Initialize a Bandit with a specified number of arms.\n\n        Args:\n            arms (List[Any]): A list representing different arms or tasks\n                              that the Bandit can choose from.\n        \"\"\"\n        self.arms: List[Any] = arms\n        self.q_values: List[float] = [0.0] * len(arms)  # Estimated rewards for each arm\n        self.counts: List[int] = [0] * len(\n            arms\n        )  # Number of times each arm has been selected\n\n    @abstractmethod\n    def select_arm(self) -&gt; int:\n        \"\"\"\n        Abstract method to select the next arm to be used.\n\n        Returns:\n            int: The index of the selected arm.\n        \"\"\"\n        pass\n\n    def update(self, arm_index: int, reward: float) -&gt; None:\n        \"\"\"\n        Update the value estimates for a given arm based on the reward received.\n\n        Args:\n            arm_index (int): Index of the arm that was selected.\n            reward (float): Reward received after selecting the arm.\n        \"\"\"\n        self.counts[arm_index] += 1\n        n = self.counts[arm_index]\n        old_q = self.q_values[arm_index]\n        self.q_values[arm_index] = ((n - 1) * old_q + reward) / n\n\n    def __repr__(self) -&gt; str:\n        \"\"\"\n        String representation of the Bandit object.\n\n        Returns:\n            str: String representation of the Bandit, showing its arms.\n        \"\"\"\n        return f\"{self.__class__.__name__}(arms={self.arms})\"\n\n    def report(self) -&gt; None:\n        \"\"\"\n        Print a report of the average rewards (Q-values) and selection counts for each arm.\n        \"\"\"\n        print(\"Q-values per arm:\")\n        for arm, q, cnt in zip(self.arms, self.q_values, self.counts):\n            print(f\"  num_tasks={arm}: avg_reward={q:.5f}, count={cnt}\")\n</code></pre> <p>               Bases: <code>Bandit</code></p> Source code in <code>lightrl/bandits.py</code> <pre><code>class EpsilonGreedyBandit(Bandit):\n    def __init__(self, arms: List[Any], epsilon: float = 0.1) -&gt; None:\n        \"\"\"\n        Initialize an EpsilonGreedyBandit with a specified number of arms and an exploration probability.\n\n        Args:\n            arms (List[Any]): A list representing different arms or tasks that the Bandit can choose from.\n            epsilon (float, optional): The probability of choosing a random arm for exploration.\n                                       Defaults to 0.1.\n        \"\"\"\n        super().__init__(arms)\n        self.epsilon: float = epsilon\n\n    def select_arm(self) -&gt; int:\n        \"\"\"\n        Select an arm to use based on the epsilon-greedy strategy.\n\n        This method uses exploration with probability 'epsilon' and exploitation otherwise,\n        selecting the arm with the highest estimated value.\n\n        Returns:\n            int: The index of the selected arm.\n        \"\"\"\n        if random.random() &lt; self.epsilon:\n            # Explore: select a random arm\n            return random.randint(0, len(self.arms) - 1)\n\n        # Exploit: select the arm with maximum estimated value\n        max_q = max(self.q_values)\n        candidates = [i for i, q in enumerate(self.q_values) if q == max_q]\n        return random.choice(candidates)\n</code></pre> <p>               Bases: <code>Bandit</code></p> Source code in <code>lightrl/bandits.py</code> <pre><code>class EpsilonFirstBandit(Bandit):\n    def __init__(\n        self, arms: List[Any], exploration_steps: int = 100, epsilon: float = 0.1\n    ) -&gt; None:\n        \"\"\"\n        Initialize an EpsilonFirstBandit with a specified number of arms, exploration steps, and exploration probability.\n\n        Args:\n            arms (List[Any]): A list representing different arms or tasks that the Bandit can choose from.\n            exploration_steps (int, optional): The number of initial steps to purely explore. Defaults to 100.\n            epsilon (float, optional): The probability of choosing a random arm during the exploration phase.\n                                       Defaults to 0.1.\n        \"\"\"\n        super().__init__(arms)\n        self.exploration_steps: int = exploration_steps\n        self.epsilon: float = epsilon\n        self.step: int = 0\n\n    def select_arm(self) -&gt; int:\n        \"\"\"\n        Select an arm to use based on the epsilon-first strategy.\n\n        This method uses pure exploration for a defined number of initial steps and then follows\n        an epsilon-greedy strategy thereafter.\n\n        Returns:\n            int: The index of the selected arm.\n        \"\"\"\n        if self.step &lt; self.exploration_steps or random.random() &lt; self.epsilon:\n            # Explore: select a random arm either during the exploration phase or if chosen randomly\n            return random.randint(0, len(self.arms) - 1)\n\n        # Exploit: select the arm with maximum estimated value\n        max_q = max(self.q_values)\n        candidates = [i for i, q in enumerate(self.q_values) if q == max_q]\n\n        self.step += 1\n        return random.choice(candidates)\n</code></pre> <p>               Bases: <code>Bandit</code></p> Source code in <code>lightrl/bandits.py</code> <pre><code>class EpsilonDecreasingBandit(Bandit):\n    def __init__(\n        self,\n        arms: List[Any],\n        initial_epsilon: float = 1.0,\n        limit_epsilon: float = 0.1,\n        half_decay_steps: int = 100,\n    ) -&gt; None:\n        \"\"\"\n        Initialize an EpsilonDecreasingBandit with a specified number of arms and epsilon parameters.\n\n        Args:\n            arms (List[Any]): A list representing different arms or tasks that the Bandit can choose from.\n            initial_epsilon (float, optional): The initial exploration probability. Defaults to 1.0.\n            limit_epsilon (float, optional): The minimum limit for the exploration probability. Defaults to 0.1.\n            half_decay_steps (int, optional): The number of steps at which the exploration probability is reduced\n                                              to half of the difference between `initial_epsilon` and `limit_epsilon`.\n                                              Defaults to 100.\n        \"\"\"\n        super().__init__(arms)\n        self.epsilon: float = initial_epsilon\n        self.initial_epsilon: float = initial_epsilon\n        self.limit_epsilon: float = limit_epsilon\n        self.half_decay_steps: int = half_decay_steps\n        self.step: int = 0\n\n    def select_arm(self) -&gt; int:\n        \"\"\"\n        Select an arm to use based on the epsilon-decreasing strategy.\n\n        This method adjusts the exploration probability over time and selects an arm accordingly.\n\n        Returns:\n            int: The index of the selected arm.\n        \"\"\"\n        self.step += 1\n        self.update_epsilon()\n\n        if random.random() &lt; self.epsilon:\n            # Explore: select a random arm\n            return random.randint(0, len(self.arms) - 1)\n        # Exploit: select the arm with maximum estimated value\n        max_q = max(self.q_values)\n        candidates = [i for i, q in enumerate(self.q_values) if q == max_q]\n        return random.choice(candidates)\n\n    def update_epsilon(self) -&gt; None:\n        \"\"\"\n        Update the exploration probability `epsilon` based on the current step.\n\n        The exploration probability decays towards the limit probability over time, according to a half-life decay model.\n        \"\"\"\n        self.epsilon = self.limit_epsilon + (\n            self.initial_epsilon - self.limit_epsilon\n        ) * (0.5 ** (self.step / self.half_decay_steps))\n</code></pre> <p>               Bases: <code>Bandit</code></p> Source code in <code>lightrl/bandits.py</code> <pre><code>class UCB1Bandit(Bandit):\n    def __init__(self, arms: List[Any]) -&gt; None:\n        \"\"\"\n        Initialize a UCB1Bandit with a specified number of arms.\n\n        Args:\n            arms (List[Any]): A list representing different arms or tasks that the Bandit can choose from.\n        \"\"\"\n        super().__init__(arms)\n        self.total_count: int = 0  # Total number of times any arm has been selected\n\n    def select_arm(self) -&gt; int:\n        \"\"\"\n        Select an arm to use based on the Upper Confidence Bound (UCB1) strategy.\n\n        This method selects an arm that maximizes the UCB estimate, accounting for exploration and exploitation.\n\n        Returns:\n            int: The index of the selected arm.\n        \"\"\"\n        for arm_index, count in enumerate(self.counts):\n            if count == 0:\n                # If an arm has not been selected yet, select it\n                return arm_index\n\n        # Calculate UCB values for each arm and choose the arm with the highest UCB value\n        ucb_values = [\n            self.q_values[i]\n            + math.sqrt((2 * math.log(self.total_count)) / self.counts[i])\n            for i in range(len(self.arms))\n        ]\n        return ucb_values.index(max(ucb_values))\n\n    def update(self, arm_index: int, reward: float) -&gt; None:\n        \"\"\"\n        Update the value estimates for a given arm based on the reward received and increment the total count.\n\n        Args:\n            arm_index (int): Index of the arm that was selected.\n            reward (float): Reward received after selecting the arm. Must be in the range [0, 1].\n\n        Raises:\n            ValueError: If the reward is not within the range [0, 1].\n        \"\"\"\n        if not (0 &lt;= reward &lt;= 1):\n            raise ValueError(\"Reward must be in the range [0, 1].\")\n        self.total_count += 1\n        super().update(arm_index, reward)\n</code></pre> <p>               Bases: <code>Bandit</code></p> Source code in <code>lightrl/bandits.py</code> <pre><code>class GreedyBanditWithHistory(Bandit):\n    def __init__(self, arms: List[Any], history_length: int = 100) -&gt; None:\n        \"\"\"\n        Initialize a GreedyBanditWithHistory with a specified number of arms and history length.\n\n        Args:\n            arms (List[Any]): A list representing different arms or tasks that the Bandit can choose from.\n            history_length (int, optional): The maximum length of history to maintain for each arm's rewards.\n                                            Defaults to 100.\n        \"\"\"\n        super().__init__(arms)\n        self.history_length: int = history_length\n        self.history: List[List[float]] = [\n            [] for _ in range(len(arms))\n        ]  # History of rewards for each arm\n\n    def select_arm(self) -&gt; int:\n        \"\"\"\n        Select an arm to use based on the greedy strategy with bounded history.\n\n        This method ensures that each arm's history reaches the defined length before purely exploiting.\n\n        Returns:\n            int: The index of the selected arm.\n        \"\"\"\n        if any(len(history) &lt; self.history_length for history in self.history):\n            # If any arm has not reached the history length, select one of these arms for exploration\n            candidates = [\n                i\n                for i, history in enumerate(self.history)\n                if len(history) &lt; self.history_length\n            ]\n            return random.choice(candidates)\n\n        # Once history length is reached for all arms, exploit the arm with maximum estimated value\n        max_q = max(self.q_values)\n        candidates = [i for i, q in enumerate(self.q_values) if q == max_q]\n        return random.choice(candidates)\n\n    def update(self, arm_index: int, reward: float) -&gt; None:\n        \"\"\"\n        Update the value estimates for a given arm based on the reward received and update its history.\n\n        Args:\n            arm_index (int): Index of the arm that was selected.\n            reward (float): Reward received after selecting the arm.\n        \"\"\"\n        if len(self.history[arm_index]) &gt;= self.history_length:\n            # Maintain bounded history by removing the oldest reward if limit is exceeded\n            self.history[arm_index].pop(0)\n        self.history[arm_index].append(reward)\n\n        # Update the count and Q-value for the arm based on its history\n        self.counts[arm_index] = len(self.history[arm_index])\n        self.q_values[arm_index] = sum(self.history[arm_index]) / self.counts[arm_index]\n</code></pre> <p>Execute a two-state time-dependent process with a bandit decision-maker.</p> <p>This function simulates a process which alternates between an \"ALIVE\" state and a \"WAITING\" state based on the performance of a given task in relation to a failure threshold. It updates the bandit model with rewards calculated from successful tasks.</p> <p>Parameters:</p> Name Type Description Default <code>bandit</code> <p>An object with methods <code>select_arm</code>, <code>update</code>, and <code>report</code>, representing     a multi-armed bandit.</p> required <code>fun</code> <code>Callable[..., Tuple[float, float]]</code> <p>A function to be called with the current arm's arguments. Should return a tuple  containing the number of successful and failed tasks.</p> required <code>failure_threshold</code> <code>float</code> <p>A float to determine what fraction of tasks fails that triggers                a switch to the \"WAITING\" state.</p> <code>0.1</code> <code>default_wait_time</code> <code>float</code> <p>The base wait time in seconds between task executions in the \"ALIVE\" state.</p> <code>5</code> <code>extra_wait_time</code> <code>float</code> <p>Additional wait time in seconds to be added in the \"WAITING\" state.</p> <code>10</code> <code>waiting_args</code> <code>Optional[Union[Tuple, List]]</code> <p>Arguments to be used when calling <code>fun</code> in the \"WAITING\" state.</p> <code>None</code> <code>max_steps</code> <code>int</code> <p>Maximum number of iterations/steps to be performed.</p> <code>500</code> <code>verbose</code> <code>bool</code> <p>If True, prints additional detailed logs and progress via tqdm.</p> <code>False</code> <code>reward_factor</code> <code>float</code> <p>A scaling factor to adjust the magnitude of the reward computed.</p> <code>1e-06</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>waiting_args</code> is not provided or if it is not of expected types.</p> Source code in <code>lightrl/runners.py</code> <pre><code>def two_state_time_dependent_process(\n    bandit,\n    fun: Callable[..., Tuple[float, float]],\n    failure_threshold: float = 0.1,\n    default_wait_time: float = 5,\n    extra_wait_time: float = 10,\n    waiting_args: Optional[Union[Tuple, List]] = None,\n    max_steps: int = 500,\n    verbose: bool = False,\n    reward_factor: float = 1e-6,\n) -&gt; None:\n    \"\"\"Execute a two-state time-dependent process with a bandit decision-maker.\n\n    This function simulates a process which alternates between an \"ALIVE\" state\n    and a \"WAITING\" state based on the performance of a given task in relation\n    to a failure threshold. It updates the bandit model with rewards calculated\n    from successful tasks.\n\n    Args:\n        bandit: An object with methods `select_arm`, `update`, and `report`, representing\n                a multi-armed bandit.\n        fun: A function to be called with the current arm's arguments. Should return a tuple\n             containing the number of successful and failed tasks.\n        failure_threshold: A float to determine what fraction of tasks fails that triggers\n                           a switch to the \"WAITING\" state.\n        default_wait_time: The base wait time in seconds between task executions in the \"ALIVE\" state.\n        extra_wait_time: Additional wait time in seconds to be added in the \"WAITING\" state.\n        waiting_args: Arguments to be used when calling `fun` in the \"WAITING\" state.\n        max_steps: Maximum number of iterations/steps to be performed.\n        verbose: If True, prints additional detailed logs and progress via tqdm.\n        reward_factor: A scaling factor to adjust the magnitude of the reward computed.\n\n    Raises:\n        ValueError: If `waiting_args` is not provided or if it is not of expected types.\n    \"\"\"\n\n    if waiting_args is None:\n        raise ValueError(\"waiting_args must be provided\")\n    else:\n        if not (isinstance(waiting_args, tuple) or isinstance(waiting_args, list)):\n            waiting_args = (waiting_args,)\n\n    state = \"ALIVE\"\n    last_alive_successes: float = 0.0\n    last_arm_index: Optional[int] = None\n    waiting_steps: int = 0\n    waiting_time: float = 0.0\n\n    iterator = range(max_steps)\n    if verbose:\n        iterator = tqdm(range(max_steps))\n\n    for _ in iterator:\n        if verbose:\n            bandit.report()\n\n        if state == \"ALIVE\":\n            current_arm_index = bandit.select_arm()\n\n            fun_args = bandit.arms[current_arm_index]\n            if not (isinstance(fun_args, tuple) or isinstance(fun_args, list)):\n                fun_args = (fun_args,)\n            successful_tasks, failed_tasks = fun(*fun_args)\n            fail_fraction = failed_tasks / (successful_tasks + failed_tasks)\n\n            time.sleep(default_wait_time)\n            waiting_time += default_wait_time\n\n            if fail_fraction &gt;= failure_threshold:\n                last_alive_successes = successful_tasks\n                last_arm_index = current_arm_index\n                state = \"WAITING\"\n                waiting_steps = 0\n            else:\n                reward = successful_tasks / waiting_time * reward_factor\n                bandit.update(current_arm_index, reward)\n                waiting_time = 0.0\n\n        else:\n            successful_tasks, failed_tasks = fun(*waiting_args)\n            fail_fraction = failed_tasks / (successful_tasks + failed_tasks)\n            waiting_steps += 1\n\n            time.sleep(default_wait_time + extra_wait_time)\n            waiting_time += default_wait_time + extra_wait_time\n\n            if fail_fraction &lt; failure_threshold:\n                reward = last_alive_successes / waiting_time * reward_factor\n                bandit.update(last_arm_index, reward)\n                waiting_time = 0.0\n                state = \"ALIVE\"\n\n    if verbose:\n        bandit.report()\n</code></pre>"},{"location":"api/#lightrl.bandits.Bandit.__init__","title":"<code>__init__(arms)</code>","text":"<p>Initialize a Bandit with a specified number of arms.</p> <p>Parameters:</p> Name Type Description Default <code>arms</code> <code>List[Any]</code> <p>A list representing different arms or tasks               that the Bandit can choose from.</p> required Source code in <code>lightrl/bandits.py</code> <pre><code>def __init__(self, arms: List[Any]) -&gt; None:\n    \"\"\"\n    Initialize a Bandit with a specified number of arms.\n\n    Args:\n        arms (List[Any]): A list representing different arms or tasks\n                          that the Bandit can choose from.\n    \"\"\"\n    self.arms: List[Any] = arms\n    self.q_values: List[float] = [0.0] * len(arms)  # Estimated rewards for each arm\n    self.counts: List[int] = [0] * len(\n        arms\n    )  # Number of times each arm has been selected\n</code></pre>"},{"location":"api/#lightrl.bandits.Bandit.__repr__","title":"<code>__repr__()</code>","text":"<p>String representation of the Bandit object.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>String representation of the Bandit, showing its arms.</p> Source code in <code>lightrl/bandits.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"\n    String representation of the Bandit object.\n\n    Returns:\n        str: String representation of the Bandit, showing its arms.\n    \"\"\"\n    return f\"{self.__class__.__name__}(arms={self.arms})\"\n</code></pre>"},{"location":"api/#lightrl.bandits.Bandit.report","title":"<code>report()</code>","text":"<p>Print a report of the average rewards (Q-values) and selection counts for each arm.</p> Source code in <code>lightrl/bandits.py</code> <pre><code>def report(self) -&gt; None:\n    \"\"\"\n    Print a report of the average rewards (Q-values) and selection counts for each arm.\n    \"\"\"\n    print(\"Q-values per arm:\")\n    for arm, q, cnt in zip(self.arms, self.q_values, self.counts):\n        print(f\"  num_tasks={arm}: avg_reward={q:.5f}, count={cnt}\")\n</code></pre>"},{"location":"api/#lightrl.bandits.Bandit.select_arm","title":"<code>select_arm()</code>  <code>abstractmethod</code>","text":"<p>Abstract method to select the next arm to be used.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The index of the selected arm.</p> Source code in <code>lightrl/bandits.py</code> <pre><code>@abstractmethod\ndef select_arm(self) -&gt; int:\n    \"\"\"\n    Abstract method to select the next arm to be used.\n\n    Returns:\n        int: The index of the selected arm.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/#lightrl.bandits.Bandit.update","title":"<code>update(arm_index, reward)</code>","text":"<p>Update the value estimates for a given arm based on the reward received.</p> <p>Parameters:</p> Name Type Description Default <code>arm_index</code> <code>int</code> <p>Index of the arm that was selected.</p> required <code>reward</code> <code>float</code> <p>Reward received after selecting the arm.</p> required Source code in <code>lightrl/bandits.py</code> <pre><code>def update(self, arm_index: int, reward: float) -&gt; None:\n    \"\"\"\n    Update the value estimates for a given arm based on the reward received.\n\n    Args:\n        arm_index (int): Index of the arm that was selected.\n        reward (float): Reward received after selecting the arm.\n    \"\"\"\n    self.counts[arm_index] += 1\n    n = self.counts[arm_index]\n    old_q = self.q_values[arm_index]\n    self.q_values[arm_index] = ((n - 1) * old_q + reward) / n\n</code></pre>"},{"location":"api/#lightrl.bandits.EpsilonGreedyBandit.__init__","title":"<code>__init__(arms, epsilon=0.1)</code>","text":"<p>Initialize an EpsilonGreedyBandit with a specified number of arms and an exploration probability.</p> <p>Parameters:</p> Name Type Description Default <code>arms</code> <code>List[Any]</code> <p>A list representing different arms or tasks that the Bandit can choose from.</p> required <code>epsilon</code> <code>float</code> <p>The probability of choosing a random arm for exploration.                        Defaults to 0.1.</p> <code>0.1</code> Source code in <code>lightrl/bandits.py</code> <pre><code>def __init__(self, arms: List[Any], epsilon: float = 0.1) -&gt; None:\n    \"\"\"\n    Initialize an EpsilonGreedyBandit with a specified number of arms and an exploration probability.\n\n    Args:\n        arms (List[Any]): A list representing different arms or tasks that the Bandit can choose from.\n        epsilon (float, optional): The probability of choosing a random arm for exploration.\n                                   Defaults to 0.1.\n    \"\"\"\n    super().__init__(arms)\n    self.epsilon: float = epsilon\n</code></pre>"},{"location":"api/#lightrl.bandits.EpsilonGreedyBandit.select_arm","title":"<code>select_arm()</code>","text":"<p>Select an arm to use based on the epsilon-greedy strategy.</p> <p>This method uses exploration with probability 'epsilon' and exploitation otherwise, selecting the arm with the highest estimated value.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The index of the selected arm.</p> Source code in <code>lightrl/bandits.py</code> <pre><code>def select_arm(self) -&gt; int:\n    \"\"\"\n    Select an arm to use based on the epsilon-greedy strategy.\n\n    This method uses exploration with probability 'epsilon' and exploitation otherwise,\n    selecting the arm with the highest estimated value.\n\n    Returns:\n        int: The index of the selected arm.\n    \"\"\"\n    if random.random() &lt; self.epsilon:\n        # Explore: select a random arm\n        return random.randint(0, len(self.arms) - 1)\n\n    # Exploit: select the arm with maximum estimated value\n    max_q = max(self.q_values)\n    candidates = [i for i, q in enumerate(self.q_values) if q == max_q]\n    return random.choice(candidates)\n</code></pre>"},{"location":"api/#lightrl.bandits.EpsilonFirstBandit.__init__","title":"<code>__init__(arms, exploration_steps=100, epsilon=0.1)</code>","text":"<p>Initialize an EpsilonFirstBandit with a specified number of arms, exploration steps, and exploration probability.</p> <p>Parameters:</p> Name Type Description Default <code>arms</code> <code>List[Any]</code> <p>A list representing different arms or tasks that the Bandit can choose from.</p> required <code>exploration_steps</code> <code>int</code> <p>The number of initial steps to purely explore. Defaults to 100.</p> <code>100</code> <code>epsilon</code> <code>float</code> <p>The probability of choosing a random arm during the exploration phase.                        Defaults to 0.1.</p> <code>0.1</code> Source code in <code>lightrl/bandits.py</code> <pre><code>def __init__(\n    self, arms: List[Any], exploration_steps: int = 100, epsilon: float = 0.1\n) -&gt; None:\n    \"\"\"\n    Initialize an EpsilonFirstBandit with a specified number of arms, exploration steps, and exploration probability.\n\n    Args:\n        arms (List[Any]): A list representing different arms or tasks that the Bandit can choose from.\n        exploration_steps (int, optional): The number of initial steps to purely explore. Defaults to 100.\n        epsilon (float, optional): The probability of choosing a random arm during the exploration phase.\n                                   Defaults to 0.1.\n    \"\"\"\n    super().__init__(arms)\n    self.exploration_steps: int = exploration_steps\n    self.epsilon: float = epsilon\n    self.step: int = 0\n</code></pre>"},{"location":"api/#lightrl.bandits.EpsilonFirstBandit.select_arm","title":"<code>select_arm()</code>","text":"<p>Select an arm to use based on the epsilon-first strategy.</p> <p>This method uses pure exploration for a defined number of initial steps and then follows an epsilon-greedy strategy thereafter.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The index of the selected arm.</p> Source code in <code>lightrl/bandits.py</code> <pre><code>def select_arm(self) -&gt; int:\n    \"\"\"\n    Select an arm to use based on the epsilon-first strategy.\n\n    This method uses pure exploration for a defined number of initial steps and then follows\n    an epsilon-greedy strategy thereafter.\n\n    Returns:\n        int: The index of the selected arm.\n    \"\"\"\n    if self.step &lt; self.exploration_steps or random.random() &lt; self.epsilon:\n        # Explore: select a random arm either during the exploration phase or if chosen randomly\n        return random.randint(0, len(self.arms) - 1)\n\n    # Exploit: select the arm with maximum estimated value\n    max_q = max(self.q_values)\n    candidates = [i for i, q in enumerate(self.q_values) if q == max_q]\n\n    self.step += 1\n    return random.choice(candidates)\n</code></pre>"},{"location":"api/#lightrl.bandits.EpsilonDecreasingBandit.__init__","title":"<code>__init__(arms, initial_epsilon=1.0, limit_epsilon=0.1, half_decay_steps=100)</code>","text":"<p>Initialize an EpsilonDecreasingBandit with a specified number of arms and epsilon parameters.</p> <p>Parameters:</p> Name Type Description Default <code>arms</code> <code>List[Any]</code> <p>A list representing different arms or tasks that the Bandit can choose from.</p> required <code>initial_epsilon</code> <code>float</code> <p>The initial exploration probability. Defaults to 1.0.</p> <code>1.0</code> <code>limit_epsilon</code> <code>float</code> <p>The minimum limit for the exploration probability. Defaults to 0.1.</p> <code>0.1</code> <code>half_decay_steps</code> <code>int</code> <p>The number of steps at which the exploration probability is reduced                               to half of the difference between <code>initial_epsilon</code> and <code>limit_epsilon</code>.                               Defaults to 100.</p> <code>100</code> Source code in <code>lightrl/bandits.py</code> <pre><code>def __init__(\n    self,\n    arms: List[Any],\n    initial_epsilon: float = 1.0,\n    limit_epsilon: float = 0.1,\n    half_decay_steps: int = 100,\n) -&gt; None:\n    \"\"\"\n    Initialize an EpsilonDecreasingBandit with a specified number of arms and epsilon parameters.\n\n    Args:\n        arms (List[Any]): A list representing different arms or tasks that the Bandit can choose from.\n        initial_epsilon (float, optional): The initial exploration probability. Defaults to 1.0.\n        limit_epsilon (float, optional): The minimum limit for the exploration probability. Defaults to 0.1.\n        half_decay_steps (int, optional): The number of steps at which the exploration probability is reduced\n                                          to half of the difference between `initial_epsilon` and `limit_epsilon`.\n                                          Defaults to 100.\n    \"\"\"\n    super().__init__(arms)\n    self.epsilon: float = initial_epsilon\n    self.initial_epsilon: float = initial_epsilon\n    self.limit_epsilon: float = limit_epsilon\n    self.half_decay_steps: int = half_decay_steps\n    self.step: int = 0\n</code></pre>"},{"location":"api/#lightrl.bandits.EpsilonDecreasingBandit.select_arm","title":"<code>select_arm()</code>","text":"<p>Select an arm to use based on the epsilon-decreasing strategy.</p> <p>This method adjusts the exploration probability over time and selects an arm accordingly.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The index of the selected arm.</p> Source code in <code>lightrl/bandits.py</code> <pre><code>def select_arm(self) -&gt; int:\n    \"\"\"\n    Select an arm to use based on the epsilon-decreasing strategy.\n\n    This method adjusts the exploration probability over time and selects an arm accordingly.\n\n    Returns:\n        int: The index of the selected arm.\n    \"\"\"\n    self.step += 1\n    self.update_epsilon()\n\n    if random.random() &lt; self.epsilon:\n        # Explore: select a random arm\n        return random.randint(0, len(self.arms) - 1)\n    # Exploit: select the arm with maximum estimated value\n    max_q = max(self.q_values)\n    candidates = [i for i, q in enumerate(self.q_values) if q == max_q]\n    return random.choice(candidates)\n</code></pre>"},{"location":"api/#lightrl.bandits.EpsilonDecreasingBandit.update_epsilon","title":"<code>update_epsilon()</code>","text":"<p>Update the exploration probability <code>epsilon</code> based on the current step.</p> <p>The exploration probability decays towards the limit probability over time, according to a half-life decay model.</p> Source code in <code>lightrl/bandits.py</code> <pre><code>def update_epsilon(self) -&gt; None:\n    \"\"\"\n    Update the exploration probability `epsilon` based on the current step.\n\n    The exploration probability decays towards the limit probability over time, according to a half-life decay model.\n    \"\"\"\n    self.epsilon = self.limit_epsilon + (\n        self.initial_epsilon - self.limit_epsilon\n    ) * (0.5 ** (self.step / self.half_decay_steps))\n</code></pre>"},{"location":"api/#lightrl.bandits.UCB1Bandit.__init__","title":"<code>__init__(arms)</code>","text":"<p>Initialize a UCB1Bandit with a specified number of arms.</p> <p>Parameters:</p> Name Type Description Default <code>arms</code> <code>List[Any]</code> <p>A list representing different arms or tasks that the Bandit can choose from.</p> required Source code in <code>lightrl/bandits.py</code> <pre><code>def __init__(self, arms: List[Any]) -&gt; None:\n    \"\"\"\n    Initialize a UCB1Bandit with a specified number of arms.\n\n    Args:\n        arms (List[Any]): A list representing different arms or tasks that the Bandit can choose from.\n    \"\"\"\n    super().__init__(arms)\n    self.total_count: int = 0  # Total number of times any arm has been selected\n</code></pre>"},{"location":"api/#lightrl.bandits.UCB1Bandit.select_arm","title":"<code>select_arm()</code>","text":"<p>Select an arm to use based on the Upper Confidence Bound (UCB1) strategy.</p> <p>This method selects an arm that maximizes the UCB estimate, accounting for exploration and exploitation.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The index of the selected arm.</p> Source code in <code>lightrl/bandits.py</code> <pre><code>def select_arm(self) -&gt; int:\n    \"\"\"\n    Select an arm to use based on the Upper Confidence Bound (UCB1) strategy.\n\n    This method selects an arm that maximizes the UCB estimate, accounting for exploration and exploitation.\n\n    Returns:\n        int: The index of the selected arm.\n    \"\"\"\n    for arm_index, count in enumerate(self.counts):\n        if count == 0:\n            # If an arm has not been selected yet, select it\n            return arm_index\n\n    # Calculate UCB values for each arm and choose the arm with the highest UCB value\n    ucb_values = [\n        self.q_values[i]\n        + math.sqrt((2 * math.log(self.total_count)) / self.counts[i])\n        for i in range(len(self.arms))\n    ]\n    return ucb_values.index(max(ucb_values))\n</code></pre>"},{"location":"api/#lightrl.bandits.UCB1Bandit.update","title":"<code>update(arm_index, reward)</code>","text":"<p>Update the value estimates for a given arm based on the reward received and increment the total count.</p> <p>Parameters:</p> Name Type Description Default <code>arm_index</code> <code>int</code> <p>Index of the arm that was selected.</p> required <code>reward</code> <code>float</code> <p>Reward received after selecting the arm. Must be in the range [0, 1].</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the reward is not within the range [0, 1].</p> Source code in <code>lightrl/bandits.py</code> <pre><code>def update(self, arm_index: int, reward: float) -&gt; None:\n    \"\"\"\n    Update the value estimates for a given arm based on the reward received and increment the total count.\n\n    Args:\n        arm_index (int): Index of the arm that was selected.\n        reward (float): Reward received after selecting the arm. Must be in the range [0, 1].\n\n    Raises:\n        ValueError: If the reward is not within the range [0, 1].\n    \"\"\"\n    if not (0 &lt;= reward &lt;= 1):\n        raise ValueError(\"Reward must be in the range [0, 1].\")\n    self.total_count += 1\n    super().update(arm_index, reward)\n</code></pre>"},{"location":"api/#lightrl.bandits.GreedyBanditWithHistory.__init__","title":"<code>__init__(arms, history_length=100)</code>","text":"<p>Initialize a GreedyBanditWithHistory with a specified number of arms and history length.</p> <p>Parameters:</p> Name Type Description Default <code>arms</code> <code>List[Any]</code> <p>A list representing different arms or tasks that the Bandit can choose from.</p> required <code>history_length</code> <code>int</code> <p>The maximum length of history to maintain for each arm's rewards.                             Defaults to 100.</p> <code>100</code> Source code in <code>lightrl/bandits.py</code> <pre><code>def __init__(self, arms: List[Any], history_length: int = 100) -&gt; None:\n    \"\"\"\n    Initialize a GreedyBanditWithHistory with a specified number of arms and history length.\n\n    Args:\n        arms (List[Any]): A list representing different arms or tasks that the Bandit can choose from.\n        history_length (int, optional): The maximum length of history to maintain for each arm's rewards.\n                                        Defaults to 100.\n    \"\"\"\n    super().__init__(arms)\n    self.history_length: int = history_length\n    self.history: List[List[float]] = [\n        [] for _ in range(len(arms))\n    ]  # History of rewards for each arm\n</code></pre>"},{"location":"api/#lightrl.bandits.GreedyBanditWithHistory.select_arm","title":"<code>select_arm()</code>","text":"<p>Select an arm to use based on the greedy strategy with bounded history.</p> <p>This method ensures that each arm's history reaches the defined length before purely exploiting.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The index of the selected arm.</p> Source code in <code>lightrl/bandits.py</code> <pre><code>def select_arm(self) -&gt; int:\n    \"\"\"\n    Select an arm to use based on the greedy strategy with bounded history.\n\n    This method ensures that each arm's history reaches the defined length before purely exploiting.\n\n    Returns:\n        int: The index of the selected arm.\n    \"\"\"\n    if any(len(history) &lt; self.history_length for history in self.history):\n        # If any arm has not reached the history length, select one of these arms for exploration\n        candidates = [\n            i\n            for i, history in enumerate(self.history)\n            if len(history) &lt; self.history_length\n        ]\n        return random.choice(candidates)\n\n    # Once history length is reached for all arms, exploit the arm with maximum estimated value\n    max_q = max(self.q_values)\n    candidates = [i for i, q in enumerate(self.q_values) if q == max_q]\n    return random.choice(candidates)\n</code></pre>"},{"location":"api/#lightrl.bandits.GreedyBanditWithHistory.update","title":"<code>update(arm_index, reward)</code>","text":"<p>Update the value estimates for a given arm based on the reward received and update its history.</p> <p>Parameters:</p> Name Type Description Default <code>arm_index</code> <code>int</code> <p>Index of the arm that was selected.</p> required <code>reward</code> <code>float</code> <p>Reward received after selecting the arm.</p> required Source code in <code>lightrl/bandits.py</code> <pre><code>def update(self, arm_index: int, reward: float) -&gt; None:\n    \"\"\"\n    Update the value estimates for a given arm based on the reward received and update its history.\n\n    Args:\n        arm_index (int): Index of the arm that was selected.\n        reward (float): Reward received after selecting the arm.\n    \"\"\"\n    if len(self.history[arm_index]) &gt;= self.history_length:\n        # Maintain bounded history by removing the oldest reward if limit is exceeded\n        self.history[arm_index].pop(0)\n    self.history[arm_index].append(reward)\n\n    # Update the count and Q-value for the arm based on its history\n    self.counts[arm_index] = len(self.history[arm_index])\n    self.q_values[arm_index] = sum(self.history[arm_index]) / self.counts[arm_index]\n</code></pre>"}]}