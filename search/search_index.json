{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the LightRL Documentation","text":"<p>Welcome to the official documentation for LightRL, a lightweight and efficient Reinforcement Learning library aimed at providing simple yet powerful tools for developing and experimenting with RL algorithms.</p>"},{"location":"#overview","title":"Overview","text":"<p>LightRL is designed to be user-friendly and flexible, providing researchers and developers with the essential tools to build and test their RL models. Whether you're a beginner or an experienced practitioner, LightRL aims to smooth the learning curve and increase productivity.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Modular Design: Easily plug and play with different algorithms and environments.</li> <li>Scalable &amp; Efficient: Optimized for performance, making it suitable for both research and production environments.</li> <li>Easy Integration: Works seamlessly with popular machine learning frameworks and libraries.</li> <li>Comprehensive Examples: Step-by-step guides and example scripts to help you get started quickly.</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>To start using LightRL, check out the following resources:</p> <ul> <li>Installation Guide: Instructions on how to install LightRL and its dependencies.</li> <li>Quick Start: A brief introduction to using LightRL for your first project.</li> <li>API Documentation: Detailed information on the API and available functions.</li> <li>Examples: In-depth tutorials and examples showcasing the features and capabilities of LightRL.</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions! Whether it\u2019s reporting a bug, suggesting new features, or contributing code, we appreciate your support. Please check out our Contributing Guidelines for more information.</p>"},{"location":"#license","title":"License","text":"<p>LightRL is licensed under the MIT License, ensuring that it remains free and open-source.</p>"},{"location":"#contact","title":"Contact","text":"<p>For questions, suggestions, or feedback, you can reach out to us at email@example.com or open an issue on our GitHub repository.</p> <p>Thank you for choosing LightRL! We hope it helps you in your journey through reinforcement learning.</p>"},{"location":"api/","title":"LightRL API Reference","text":"<p>Welcome to the detailed API reference for LightRL. Below you'll find documentation for the key classes and functions available in the library, complete with usage guidelines and examples.</p>"},{"location":"api/#bandits-module","title":"Bandits Module","text":"<p>LightRL includes a variety of bandit algorithms, each tailored for specific use cases in reinforcement learning environments. The following classes are part of the <code>lightrl.bandits</code> module:</p>"},{"location":"api/#base-bandit-class","title":"Base Bandit Class","text":"<ul> <li><code>Bandit</code>: The foundational class for all bandit algorithms. Subclasses provide specialized implementations.   ::: lightrl.bandits.Bandit</li> </ul>"},{"location":"api/#epsilon-based-bandits","title":"Epsilon-Based Bandits","text":"<p>These bandits use epsilon strategies to balance exploration and exploitation.</p> <ul> <li> <p><code>EpsilonGreedyBandit</code>: Implements an epsilon-greedy algorithm, allowing for a tunable exploration rate.   ::: lightrl.bandits.EpsilonGreedyBandit</p> </li> <li> <p><code>EpsilonFirstBandit</code>: Prioritizes exploration for a set number of initial steps before switching to exploitation.   ::: lightrl.bandits.EpsilonFirstBandit</p> </li> <li> <p><code>EpsilonDecreasingBandit</code>: Uses a decreasing epsilon value over time to reduce exploration as understanding improves.   ::: lightrl.bandits.EpsilonDecreasingBandit</p> </li> </ul>"},{"location":"api/#other-bandit-strategies","title":"Other Bandit Strategies","text":"<ul> <li> <p><code>UCB1Bandit</code>: Employs the UCB1 algorithm, focusing on arm pulls with calculated confidence bounds.   ::: lightrl.bandits.UCB1Bandit</p> </li> <li> <p><code>GreedyBanditWithHistory</code>: A variant that uses historical performance data to adjust its greedy selection strategy.   ::: lightrl.bandits.GreedyBanditWithHistory</p> </li> </ul>"},{"location":"api/#runners-module","title":"Runners Module","text":"<ul> <li><code>two_state_time_dependent_process</code>: This function models a process with time-dependent state transitions, useful in simulating dynamic environments.   ::: lightrl.runners.two_state_time_dependent_process</li> </ul> <p>If you have any questions or require further assistance, feel free to open an issue.</p>"}]}